{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Katyapuchkova1/Practice/blob/main/%D0%9F%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D0%BA%D0%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "!pip install python-pptx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bB2RrojpPqaH",
        "outputId": "822e000a-ac68-4238-891b-67c834c94b26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/232.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting python-pptx\n",
            "  Downloading python_pptx-0.6.23-py3-none-any.whl (471 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (4.9.4)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (9.4.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx)\n",
            "  Downloading XlsxWriter-3.1.9-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: XlsxWriter, python-pptx\n",
            "Successfully installed XlsxWriter-3.1.9 python-pptx-0.6.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "C0L713GNyyxy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8qdxKbsOIZT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "d7a9a512-7d75-4527-acb8-8c68f4a691f1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/4'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9d1e27ec80ce>\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0monlyfiles4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'4/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/4'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'4'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0monlyfiles3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'3/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/3'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'3'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0monlyfiles2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'2/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/2'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'2'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/4'"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
        "from PyPDF2 import PdfReader\n",
        "import codecs\n",
        "from pptx import Presentation\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "\n",
        "def txt_from_pdf(filename_pdf):\n",
        "  '''Перевод из pdf в txt и сохранение в память'''\n",
        "  reader = PdfReader(\"/content/\" + filename_pdf)\n",
        "  number_of_pages = len(reader.pages)\n",
        "  text = ''\n",
        "  for page_num in range(number_of_pages):\n",
        "    text += reader.pages[page_num].extract_text()\n",
        "  with open(filename_pdf[:-3] + 'txt', 'w') as f:\n",
        "    f.write(text)\n",
        "  return None\n",
        "\n",
        "#Перевод из pptx в txt формат\n",
        "\n",
        "def txt_from_pptx(filename_pptx):\n",
        "    '''Перевод из pptx в txt и сохранение в память'''\n",
        "    X = Presentation(\"/content/\" + filename_pptx)  # Presentation object created\n",
        "    # Then file is opened in write mode\n",
        "    ftw_data = open(filename_pptx[:-4] + 'txt', \"w\")\n",
        "    # write text from powerpoint\n",
        "    # file into .txt file\n",
        "    for slide in X.slides:\n",
        "        for shape in slide.shapes:\n",
        "            if not shape.has_text_frame:\n",
        "                continue\n",
        "            for paragraph in shape.text_frame.paragraphs:\n",
        "                for run in paragraph.runs:\n",
        "                    ftw_data.write(run.text)\n",
        "    ftw_data.close()  # The file is closed\n",
        "\n",
        "\n",
        "onlyfiles4 = ['4/' + f for f in listdir('/content/4') if isfile(join('/content/' + '4' + '/', f))]\n",
        "onlyfiles3 = ['3/' + f for f in listdir('/content/3') if isfile(join('/content/' + '3' + '/', f))]\n",
        "onlyfiles2 = ['2/' + f for f in listdir('/content/2') if isfile(join('/content/' + '2' + '/', f))]\n",
        "onlyfiles1 = ['1/' + f for f in listdir('/content/1') if isfile(join('/content/' + '1' + '/', f))]\n",
        "for f in onlyfiles2:\n",
        "    txt_from_pptx(f)\n",
        "\n",
        "\n",
        "letters = ['й, ц, у, к, е, н, г, ш, щ, з, х, ъ, ф, ы, в, а, п, р, о, л, д, ж, э, я, ч, с, м, и, т, ь, б, ю']\n",
        "vowels = ['а', 'е', 'ы', 'у', 'э', 'о', 'я', 'и', 'ю']\n",
        "punctuation = [',', ';', '.', ':', '?', ')', '(', '!']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "io3MTGSny6iD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Получение словаря сложных слов\n",
        "tokenizer=RegexpTokenizer(r'\\n\\w{4,}\\n')\n",
        "url_diff = 'https://mydocx.ru/9-118219.html'\n",
        "vocab = requests.get(url_diff)\n",
        "soup = BeautifulSoup(vocab.content, 'html.parser')\n",
        "text_vocab = soup.get_text()\n",
        "vocab_del = tokenizer.tokenize(text_vocab.lower())\n",
        "vocab = map(lambda x: x.strip('\\n'), vocab_del)\n",
        "vocab_dif = list(vocab)[1:]"
      ],
      "metadata": {
        "id": "bo0c3NrwELcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CONSTANT_SCALING = 4\n",
        "#Метрики удобочитаемости\n",
        "\n",
        "def calc_Flesh_Kincaid_rus(n_syllabes, n_words, n_sent):\n",
        "    \"\"\"Метрика Flesh Kincaid для русского языка\"\"\"\n",
        "    n = 220.755 - 1.315 * (float(n_words) / n_sent) - 50.1 * (float(n_syllabes) / n_words)\n",
        "    return n/CONSTANT_SCALING\n",
        "\n",
        "def calc_Flesh_Kincaid_Grade_rus(n_syllabes, n_words, n_sent):\n",
        "    \"\"\"Метрика Flesh Kincaid Grade для русского языка\"\"\"\n",
        "#    n = 0.59 * (float(n_words) / n_sent) + 6.2 * (float(n_syllabes) / n_words) - 16.59\n",
        "    n = 0.49 * (float(n_words) / n_sent) + 7.3 * (float(n_syllabes) / n_words) - 16.59\n",
        "    return n/CONSTANT_SCALING\n",
        "\n",
        "FLG_X_GRADE = 0.318\n",
        "FLG_Y_GRADE = 14.2\n",
        "FLG_Z_GRADE = 30.5\n",
        "\n",
        "def calc_Flesh_Kincaid_Grade_rus_flex(n_syllabes, n_words, n_sent):\n",
        "    \"\"\"Метрика Flesh Kincaid Grade для русского языка с константными параметрами\"\"\"\n",
        "    if n_words == 0 or n_sent == 0: return 0\n",
        "    n = FLG_X_GRADE * (float(n_words) / n_sent) + FLG_Y_GRADE * (float(n_syllabes) / n_words) - FLG_Z_GRADE\n",
        "    return n/CONSTANT_SCALING\n",
        "\n",
        "CLI_X_GRADE = 0.055\n",
        "CLI_Y_GRADE = 0.35\n",
        "CLI_Z_GRADE = 20.33\n",
        "\n",
        "\n",
        "def calc_Coleman_Liau_index(n_letters, n_words, n_sent):\n",
        "    \"\"\" Метрика Coleman Liau для русского языка с константными параметрами \"\"\"\n",
        "    if n_words == 0: return 0\n",
        "    n = CLI_X_GRADE * (n_letters * (100.0 / n_words)) - CLI_Y_GRADE * (n_sent * (100.0 / n_words)) - CLI_Z_GRADE\n",
        "    return n/CONSTANT_SCALING\n",
        "\n",
        "SMOG_X_GRADE = 1.\n",
        "SMOG_Y_GRADE = 64.6\n",
        "SMOG_Z_GRADE = 0.05\n",
        "\n",
        "def calc_SMOG_index(n_psyl, n_sent):\n",
        "    \"\"\"Метрика SMOG для русского языка с константными параментрами\"\"\"\n",
        "    n = SMOG_X_GRADE * sqrt((float(SMOG_Y_GRADE) / n_sent) * n_psyl) + SMOG_Z_GRADE\n",
        "    return n/CONSTANT_SCALING\n",
        "\n",
        "DC_X_GRADE = 0.552\n",
        "DC_Y_GRADE = 0.273\n",
        "\n",
        "def calc_Dale_Chale_index(n_psyl, n_words, n_sent):\n",
        "    \"\"\"Метрика Dale Chale для русского языка с константным параметрами\"\"\"\n",
        "    n = DC_X_GRADE * (100.0 * n_psyl / n_words) + DC_Y_GRADE * (float(n_words) / n_sent)\n",
        "    return n/CONSTANT_SCALING\n",
        "\n",
        "ARI_X_GRADE = 6.26\n",
        "ARI_Y_GRADE = 0.2805\n",
        "ARI_Z_GRADE = 31.04\n",
        "\n",
        "\n",
        "def calc_ARI_index(n_letters, n_words, n_sent):\n",
        "    \"\"\" Метрика Automated Readability Index (ARI) для русского языка с константными параметрами \"\"\"\n",
        "    if n_words == 0 or n_sent == 0: return 0\n",
        "    n = ARI_X_GRADE * (float(n_letters) / n_words) + ARI_Y_GRADE * (float(n_words) / n_sent) - ARI_Z_GRADE\n",
        "    return n/CONSTANT_SCALING"
      ],
      "metadata": {
        "id": "AlrMspOph_gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgwoNTkE6k29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "570b5618-3631-4288-cced-b06a00d3234c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "from string import punctuation\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "#Подсчет характеристик для формул\n",
        "def count_metrics(text):\n",
        "  \"\"\"Подсчет основных характеристик текста\"\"\"\n",
        "  sents = sent_tokenize(text)\n",
        "  num_sent = len(sents)\n",
        "  num_words = 0\n",
        "  num_letters = 0\n",
        "  num_syl = 0\n",
        "  num_hardwords = 0\n",
        "  for sent in sents:\n",
        "    words = word_tokenize(sent)\n",
        "    words_cleaned =  list(filter(lambda x: x not in punctuation, words))\n",
        "    num_words += len(words_cleaned)\n",
        "    for word in words_cleaned:\n",
        "      if word in vocab_dif: num_hardwords += 1\n",
        "      num_letters += len(word)\n",
        "      for letter in word:\n",
        "        if letter in vowels: num_syl += 1\n",
        "  return (num_sent, num_words, num_letters, num_syl, num_hardwords)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}